{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1Predicting sales data using Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, decode, expr\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession  # Spark SQL\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "# the below setup will run Spark in local mode with * working processors(equal to logical cores on the machine)\n",
    "master = \"local[4]\"\n",
    "\n",
    "# Setup `appName` field to be displayed at Spark cluster UI page\n",
    "app_name = \"FIT5202 Assignment 2b\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = (SparkConf()\n",
    "              .setMaster(master)\n",
    "              .setAppName(app_name))\n",
    "\n",
    "# Setup SparkSession and configure it with Melbourne timezone.\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=spark_conf)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define schema and load file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide a schema to the valuees\n",
    "produce_data_labels = [\n",
    "    (\"Store\", StringType()),\n",
    "    (\"Date\", StringType()),\n",
    "    (\"Temperature\", StringType()),\n",
    "    (\"Fuel_Price\", StringType()),\n",
    "    (\"MarkDown1\", StringType()),\n",
    "    (\"MarkDown2\", StringType()),\n",
    "    (\"MarkDown3\", StringType()),\n",
    "    (\"MarkDown4\", StringType()),\n",
    "    (\"MarkDown5\", StringType()),\n",
    "    (\"CPI\", StringType()),\n",
    "    (\"Unemployment\", StringType()),\n",
    "    (\"IsHoliday\", StringType()),\n",
    "    (\"last_weekly_sales\", StringType()),\n",
    "    (\"ts\", IntegerType())\n",
    "]\n",
    "\n",
    "# features schema\n",
    "produce_data_schema = ArrayType(StructType(\n",
    "    [StructField(x[0], x[1], True) for x in produce_data_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Injest Kafka data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "hostip = \"192.168.8.133\"  # change me\n",
    "topic = 'assignment2b'\n",
    "\n",
    "# read df\n",
    "df = (spark\n",
    "      .readStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", f'{hostip}:9092')\n",
    "      .option(\"subscribe\", topic)\n",
    "      .option(\"dateFormat\", \"d/M/y\")\n",
    "      .load()  # load df\n",
    "      # re-hydrate binary\n",
    "      .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "      .select(F.from_json(F.col(\"value\").cast(\"string\"),\n",
    "                          produce_data_schema).alias('parsed_value'))  # parse json\n",
    "      # un-nest columns\n",
    "      .select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))\n",
    "      )\n",
    "\n",
    "# format dataframe and cast into proper data types\n",
    "df_formatted = df.select(\n",
    "    F.col(\"unnested_value.Store\").alias(\"Store\"),\n",
    "    F.col(\"unnested_value.Date\").alias(\"Date\"),\n",
    "    F.col(\"unnested_value.Temperature\").alias(\"Temperature\"),\n",
    "    F.col(\"unnested_value.Fuel_Price\").alias(\"Fuel_Price\"),\n",
    "    F.col(\"unnested_value.MarkDown1\").alias(\"MarkDown1\"),\n",
    "    F.col(\"unnested_value.MarkDown2\").alias(\"MarkDown2\"),\n",
    "    F.col(\"unnested_value.MarkDown3\").alias(\"MarkDown3\"),\n",
    "    F.col(\"unnested_value.MarkDown4\").alias(\"MarkDown4\"),\n",
    "    F.col(\"unnested_value.MarkDown5\").alias(\"MarkDown5\"),\n",
    "    F.col(\"unnested_value.CPI\").alias(\"CPI\"),\n",
    "    F.col(\"unnested_value.Unemployment\").alias(\"Unemployment\"),\n",
    "    F.col(\"unnested_value.IsHoliday\").alias(\"IsHoliday\"),\n",
    "    F.col(\"unnested_value.last_weekly_sales\").alias(\"last_weekly_sales\"),\n",
    "    F.col(\"unnested_value.ts\").alias(\"ts\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Persist raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "|Store|Date      |Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|CPI      |Unemployment|IsHoliday|last_weekly_sales |ts        |\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "|5    |2011-09-02|90.38      |3.533     |nan      |nan      |nan      |nan      |nan      |216.35886|6.529       |false    |310338.1683688164 |1675494330|\n",
      "|12   |2011-09-02|93.66      |3.798     |nan      |nan      |nan      |nan      |nan      |129.32594|13.503      |false    |1017593.4658427238|1675494330|\n",
      "|43   |2011-09-02|87.84      |3.533     |nan      |nan      |nan      |nan      |nan      |207.6207 |10.641      |false    |561573.0730142593 |1675494330|\n",
      "|33   |2011-09-02|99.2       |3.798     |nan      |nan      |nan      |nan      |nan      |129.32594|8.442       |false    |237095.82024645805|1675494330|\n",
      "|9    |2011-09-02|89.33      |3.533     |nan      |nan      |nan      |nan      |nan      |219.38239|6.404       |false    |542663.532877922  |1675494330|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "|Store|Date      |Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|CPI      |Unemployment|IsHoliday|last_weekly_sales |ts        |\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "|17   |2011-01-21|26.62      |2.934     |nan      |nan      |nan      |nan      |nan      |127.44048|6.866       |false    |758510.348657608  |1675495121|\n",
      "|28   |2011-01-21|53.53      |3.223     |nan      |nan      |nan      |nan      |nan      |127.44048|14.021      |false    |1098286.6037750244|1675495121|\n",
      "|11   |2011-01-21|51.51      |3.016     |nan      |nan      |nan      |nan      |nan      |215.12683|7.551       |false    |1194449.7854175568|1675495121|\n",
      "|45   |2011-01-21|30.55      |3.229     |nan      |nan      |nan      |nan      |nan      |182.91934|8.549       |false    |654018.9482831955 |1675495121|\n",
      "|1    |2011-01-21|44.04      |3.016     |nan      |nan      |nan      |nan      |nan      |211.82724|7.742       |false    |1391013.9626731873|1675495121|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final dataframe\n",
    "query = (df_formatted\n",
    "         .writeStream\n",
    "         .format(\"parquet\")\n",
    "         .option(\"path\", \"output/filesink_output\")\n",
    "         .option(\"checkpointLocation\", \"checkpoint/filesink_checkpoint\")\n",
    "         .foreachBatch(foreach_batch_function)\n",
    "         .trigger(processingTime='5 seconds')\n",
    "         .start()\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop query\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Prepare feature columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dataframe and cast into proper data types\n",
    "df_final = (df_formatted\n",
    "            # cast data types\n",
    "            .select(\n",
    "                F.col(\"Store\").cast(StringType()),\n",
    "                F.col(\"Date\").cast(DateType()),\n",
    "                F.col(\"Temperature\").cast(FloatType()),\n",
    "                F.col(\"Fuel_Price\").cast(FloatType()),\n",
    "                F.col(\"MarkDown1\").cast(FloatType()),\n",
    "                F.col(\"MarkDown2\").cast(FloatType()),\n",
    "                F.col(\"MarkDown3\").cast(FloatType()),\n",
    "                F.col(\"MarkDown4\").cast(FloatType()),\n",
    "                F.col(\"MarkDown5\").cast(FloatType()),\n",
    "                F.col(\"CPI\").cast(FloatType()),\n",
    "                F.col(\"Unemployment\").cast(FloatType()),\n",
    "                F.col(\"IsHoliday\").cast(StringType()),\n",
    "                F.col(\"last_weekly_sales\").cast(FloatType()),\n",
    "                F.col(\"ts\").cast(TimestampType()))\n",
    "            #create new columns\n",
    "            .withColumn(\"Month\", F.month(\"Date\"))\n",
    "            .withColumn(\"day_of_month\", F.dayofmonth(\"Date\"))\n",
    "            .withColumn(\"day_of_year\", F.dayofyear(\"Date\"))\n",
    "            .withColumn(\"week_of_year\", F.weekofyear(\"Date\"))\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = (df_final\n",
    "#          .writeStream\n",
    "#          .outputMode(\"append\")\n",
    "#          .format(\"console\")\n",
    "#          .option(\"truncate\", False)\n",
    "#          # send the above dataframe to console every 5 seconds\n",
    "#          .trigger(processingTime='5 seconds')\n",
    "#          .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Join the local data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read stores dataset\n",
    "\n",
    "# stores data type\n",
    "stores_labels = [\n",
    "    (\"Store\", StringType()),\n",
    "    (\"Type\", StringType()),\n",
    "    (\"Size\", IntegerType()),\n",
    "]\n",
    "# stores schema\n",
    "stores_schema = StructType([StructField(x[0], x[1], True)\n",
    "                           for x in stores_labels])\n",
    "\n",
    "# load stores df\n",
    "df_stores = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .load(\"data/stores.csv\", schema=stores_schema)\n",
    ")\n",
    "\n",
    "df_joined = (df_final\n",
    "            .join(df_stores,df_final.Store==df_stores.Store,how=\"left\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (df_joined\n",
    "         .select(\"Type\",\"Size\")\n",
    "         .writeStream\n",
    "         .outputMode(\"append\")\n",
    "         .format(\"console\")\n",
    "         .option(\"truncate\", False)\n",
    "         # send the above dataframe to console every 5 seconds\n",
    "         .trigger(processingTime='5 seconds')\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Perform predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load persisted model\n",
    "pipelineModel = PipelineModel.load('sales_estimation_pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\tGBTRegressor_428b57e2311a-featuresCol: features,\n",
      "\tGBTRegressor_428b57e2311a-labelCol: Weekly_Sales\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(pipelineModel.stages[-1]._java_obj.paramMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 write code to process the data following requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 average weekly sales predictions of different types of stores and write the stream back to Kafka sink using a different topic name\n",
    "\n",
    "The data you sended should be like this:\n",
    "\n",
    "|  key   | value  |\n",
    "|  ----  | ----  |\n",
    "| timestamp of window start | JSON of store type and avg sales |\n",
    "| '1673233646'  | '{\"Type\":\"A\",\"predict_weekly_sales\":20000}' |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### For cleaning up the quries and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
