{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1Predicting sales data using Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create SparkSession\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, decode, expr, window\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession  # Spark SQL\n",
    "import os\n",
    "import json\n",
    "from json import dumps\n",
    "from kafka3 import KafkaProducer\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "# the below setup will run Spark in local mode with * working processors(equal to logical cores on the machine)\n",
    "master = \"local[4]\"\n",
    "\n",
    "# Setup `appName` field to be displayed at Spark cluster UI page\n",
    "app_name = \"FIT5202 Assignment 2b\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = (SparkConf().setMaster(master).set(\n",
    "    \"spark.sql.streaming.checkpointLocation\",\n",
    "    'data/streaming_checkpoint')\n",
    "    .setAppName(app_name)\n",
    ")\n",
    "\n",
    "# Setup SparkSession and configure it with Melbourne timezone.\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=spark_conf)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define schema and load file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide a schema to the valuees\n",
    "produce_data_labels = [\n",
    "    (\"Store\", StringType()),\n",
    "    (\"Date\", StringType()),\n",
    "    (\"Temperature\", StringType()),\n",
    "    (\"Fuel_Price\", StringType()),\n",
    "    (\"MarkDown1\", StringType()),\n",
    "    (\"MarkDown2\", StringType()),\n",
    "    (\"MarkDown3\", StringType()),\n",
    "    (\"MarkDown4\", StringType()),\n",
    "    (\"MarkDown5\", StringType()),\n",
    "    (\"CPI\", StringType()),\n",
    "    (\"Unemployment\", StringType()),\n",
    "    (\"IsHoliday\", StringType()),\n",
    "    (\"last_weekly_sales\", StringType()),\n",
    "    (\"ts\", LongType())\n",
    "]\n",
    "\n",
    "# features schema\n",
    "produce_data_schema = ArrayType(StructType(\n",
    "    [StructField(x[0], x[1], True) for x in produce_data_labels]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Injest Kafka data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "hostip = \"192.168.8.133\"  # change me\n",
    "topic = 'assignment2b'\n",
    "\n",
    "# read df\n",
    "df = (spark\n",
    "      .readStream\n",
    "      .format(\"kafka\")\n",
    "      .option(\"kafka.bootstrap.servers\", f'{hostip}:9092')\n",
    "      .option(\"subscribe\", topic)\n",
    "      .option(\"dateFormat\", \"d/M/y\")\n",
    "      .load()\n",
    "      # re-hydrate binary\n",
    "      .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "      .select(F.from_json(F.col(\"value\").cast(\"string\"),\n",
    "                          produce_data_schema).alias('parsed_value'))  # parse json\n",
    "      # un-nest columns\n",
    "      .select(F.explode(F.col(\"parsed_value\")).alias('unnested_value'))\n",
    "      )\n",
    "\n",
    "# format dataframe and cast into proper data types\n",
    "df_formatted = df.select(\n",
    "    F.col(\"unnested_value.Store\").alias(\"Store\"),\n",
    "    F.col(\"unnested_value.Date\").alias(\"Date\"),\n",
    "    F.col(\"unnested_value.Temperature\").alias(\"Temperature\"),\n",
    "    F.col(\"unnested_value.Fuel_Price\").alias(\"Fuel_Price\"),\n",
    "    F.col(\"unnested_value.MarkDown1\").alias(\"MarkDown1\"),\n",
    "    F.col(\"unnested_value.MarkDown2\").alias(\"MarkDown2\"),\n",
    "    F.col(\"unnested_value.MarkDown3\").alias(\"MarkDown3\"),\n",
    "    F.col(\"unnested_value.MarkDown4\").alias(\"MarkDown4\"),\n",
    "    F.col(\"unnested_value.MarkDown5\").alias(\"MarkDown5\"),\n",
    "    F.col(\"unnested_value.CPI\").alias(\"CPI\"),\n",
    "    F.col(\"unnested_value.Unemployment\").alias(\"Unemployment\"),\n",
    "    F.col(\"unnested_value.IsHoliday\").alias(\"IsHoliday\"),\n",
    "    F.col(\"unnested_value.last_weekly_sales\").alias(\"last_weekly_sales\"),\n",
    "    F.col(\"unnested_value.ts\").alias(\"ts\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Persist raw data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "|Store|Date      |Temperature|Fuel_Price|MarkDown1|MarkDown2|MarkDown3|MarkDown4|MarkDown5|CPI      |Unemployment|IsHoliday|last_weekly_sales |ts        |\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "|17   |2011-01-21|26.62      |2.934     |nan      |nan      |nan      |nan      |nan      |127.44048|6.866       |false    |758510.348657608  |1675676397|\n",
      "|28   |2011-01-21|53.53      |3.223     |nan      |nan      |nan      |nan      |nan      |127.44048|14.021      |false    |1098286.6037750244|1675676397|\n",
      "|11   |2011-01-21|51.51      |3.016     |nan      |nan      |nan      |nan      |nan      |215.12683|7.551       |false    |1194449.7854175568|1675676397|\n",
      "|45   |2011-01-21|30.55      |3.229     |nan      |nan      |nan      |nan      |nan      |182.91934|8.549       |false    |654018.9482831955 |1675676397|\n",
      "|1    |2011-01-21|44.04      |3.016     |nan      |nan      |nan      |nan      |nan      |211.82724|7.742       |false    |1391013.9626731873|1675676397|\n",
      "+-----+----------+-----------+----------+---------+---------+---------+---------+---------+---------+------------+---------+------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# final dataframe\n",
    "query_parquet = (df_formatted\n",
    "                 .writeStream\n",
    "                 .format(\"parquet\")\n",
    "                 .option(\"path\", \"data/parquet_output\")\n",
    "                 .option(\"checkpointLocation\", \"data/parquet_output/checkpoint\")\n",
    "                 .foreachBatch(foreach_batch_function)\n",
    "                 .start()\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop query\n",
    "query_parquet.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Prepare feature columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format dataframe and cast into proper data types\n",
    "df_final = (df_formatted\n",
    "            # cast data types\n",
    "            .select(\n",
    "                F.col(\"Store\").cast(IntegerType()),\n",
    "                F.col(\"Date\").cast(DateType()),\n",
    "                F.col(\"Temperature\").cast(FloatType()),\n",
    "                F.col(\"Fuel_Price\").cast(FloatType()),\n",
    "                F.col(\"MarkDown1\").cast(FloatType()),\n",
    "                F.col(\"MarkDown2\").cast(FloatType()),\n",
    "                F.col(\"MarkDown3\").cast(FloatType()),\n",
    "                F.col(\"MarkDown4\").cast(FloatType()),\n",
    "                F.col(\"MarkDown5\").cast(FloatType()),\n",
    "                F.col(\"CPI\").cast(FloatType()),\n",
    "                F.col(\"Unemployment\").cast(FloatType()),\n",
    "                F.col(\"IsHoliday\").cast(IntegerType()),\n",
    "                F.col(\"last_weekly_sales\").cast(FloatType()),\n",
    "                F.col(\"ts\").cast(TimestampType()))\n",
    "            # create new columns\n",
    "            .withColumn(\"Month\", F.month(\"Date\"))\n",
    "            .withColumn(\"day_of_month\", F.dayofmonth(\"Date\"))\n",
    "            .withColumn(\"day_of_year\", F.dayofyear(\"Date\"))\n",
    "            .withColumn(\"week_of_year\", F.weekofyear(\"Date\"))\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Join the local data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read stores dataset\n",
    "\n",
    "# stores data type\n",
    "stores_labels = [\n",
    "    (\"Store\", IntegerType()),\n",
    "    (\"Type\", StringType()),\n",
    "    (\"Size\", IntegerType()),\n",
    "]\n",
    "# stores schema\n",
    "stores_schema = StructType([StructField(x[0], x[1], True)\n",
    "                           for x in stores_labels])\n",
    "\n",
    "# load stores df\n",
    "df_stores = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .load(\"data/stores.csv\", schema=stores_schema)\n",
    ")\n",
    "\n",
    "df_joined = (df_final\n",
    "             .join(df_stores, df_final.Store == df_stores.Store, how=\"left\")\n",
    "             .drop(df_final.Store)\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Perform predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml import PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load persisted model\n",
    "pipelineModel = PipelineModel.load('sales_estimation_pipeline_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create predictions df\n",
    "predictions_df = pipelineModel.transform(df_joined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 write code to process the data following requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create a grouped query\n",
    "query_29 = (predictions_df\n",
    "            # transformations\n",
    "            .withColumn(\"achieve_goal\", col('last_weekly_sales')/col('Size'))\n",
    "            .filter(col(\"achieve_goal\") > 8.5)\n",
    "            .withWatermark(\"ts\", \"3 seconds\")\n",
    "            .groupby(window(col(\"ts\"),\n",
    "                            windowDuration=\"10 seconds\",\n",
    "                            slideDuration=\"5 seconds\"),\n",
    "                     col(\"Type\"))\n",
    "            .agg(F.count(\"Store\").alias(\"count\"))\n",
    "            # query parameters\n",
    "            .writeStream\n",
    "            .outputMode(\"update\")\n",
    "            .format(\"console\")\n",
    "            .option(\"truncate\", False)\n",
    "            # drop empty rows\n",
    "            .option(\"dropEmptyRows\", \"true\")\n",
    "            # send the above dataframe to console every 5 seconds\n",
    "            .trigger(processingTime='5 seconds')\n",
    "            .start()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_29.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially write a foreach that filters out and only leave the last 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10 average weekly sales predictions of different types of stores and write the stream back to Kafka sink using a different topic name\n",
    "\n",
    "The data you sended should be like this:\n",
    "\n",
    "|  key   | value  |\n",
    "|  ----  | ----  |\n",
    "| timestamp of window start | JSON of store type and avg sales |\n",
    "| '1673233646'  | '{\"Type\":\"A\",\"predict_weekly_sales\":20000}' |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function to process the data and write it back to the Kafka topic\n",
    "def process_batch2(df,epoch_id):\n",
    "    # Convert the processed data to a dict and serialize it as a string\n",
    "    data = [{\"key\": row.key, \"value\": row.value} for row in df.collect()]\n",
    "    serialized_data = json.dumps(data)\n",
    "    # Write the serialized data back to the Kafka topic\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[f'{hostip}:9092'],\n",
    "        value_serializer=lambda x: x.encode(\"ascii\"),\n",
    "    )\n",
    "    producer.send(\"test_out\", value=serialized_data)\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to process the data and write it back to the Kafka topic\n",
    "def process_batch3(df,epoch_id):\n",
    "    # Convert the processed data to a dict and serialize it as a string\n",
    "    data = [{\"key\": row.key, \"value\": row.value} for row in df]\n",
    "    serialized_data = json.dumps(data)\n",
    "    # Write the serialized data back to the Kafka topic\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=[f'{hostip}:9092'],\n",
    "        value_serializer=lambda x: x.encode(\"ascii\"),\n",
    "    )\n",
    "    producer.send(\"test_out\", value=serialized_data)\n",
    "    producer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to create a grouped query\n",
    "# test\n",
    "streaming_df = (predictions_df\n",
    "                # transformations\n",
    "                .withWatermark(\"ts\", \"3 seconds\")\n",
    "                .groupby(window(col(\"ts\"),\n",
    "                                windowDuration=\"10 seconds\",\n",
    "                                slideDuration=\"5 seconds\"),\n",
    "                         col(\"Type\"))\n",
    "                .agg(F.mean(\"prediction\").alias(\"predict_weekly_sales\"))\n",
    "                .withColumn(\"key\", F.unix_timestamp(\"window.start\"))\n",
    "                .withColumn(\"value\", F.to_json(F.struct(\"Type\", \"predict_weekly_sales\")))\n",
    "                .select(\"key\", \"value\")\n",
    "                .selectExpr(\n",
    "                    \"cast(key as string) as key\",\n",
    "                    \"cast(value as string) as value\")\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery = (streaming_df\n",
    "                  .writeStream\n",
    "                  .foreachBatch(process_batch3)\n",
    "                  .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "streamingQuery.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/conda/lib/python3.8/site-packages/py4j/clientserver.py\", line 617, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 272, in call\\n    raise e\\n  File \"/opt/conda/lib/python3.8/site-packages/pyspark/sql/utils.py\", line 269, in call\\n    self.func(DataFrame(jdf, self.session), batch_id)\\n  File \"/tmp/ipykernel_101/3418698970.py\", line 5, in process_batch3\\n    serialized_data = json.dumps(data)\\n  File \"/opt/conda/lib/python3.8/json/__init__.py\", line 231, in dumps\\n    return _default_encoder.encode(obj)\\n  File \"/opt/conda/lib/python3.8/json/encoder.py\", line 199, in encode\\n    chunks = self.iterencode(o, _one_shot=True)\\n  File \"/opt/conda/lib/python3.8/json/encoder.py\", line 257, in iterencode\\n    return _iterencode(o, 0)\\n  File \"/opt/conda/lib/python3.8/json/encoder.py\", line 179, in default\\n    raise TypeError(f\\'Object of type {o.__class__.__name__} \\'\\nTypeError: Object of type Column is not JSON serializable\\n',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streamingQuery.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
